"""
Trend Tracker v1.0
è¶‹åŠ¿è¿½è¸ªå¼•æ“ â€” å®æ—¶çƒ­ç‚¹æ£€æµ‹ + ç›¸å…³åº¦è¯„ä¼° + å‚ä¸æ—¶æœºå»ºè®®

Features:
- TrendCollector: å¤šæºè¶‹åŠ¿é‡‡é›† (hashtagé¢‘æ¬¡, è¯é¢˜çˆ†å‘, å…³é”®è¯çªå¢)
- BurstDetector: çˆ†å‘æ£€æµ‹ç®—æ³• (ç§»åŠ¨çª—å£ + Z-scoreå¼‚å¸¸æ£€æµ‹)
- RelevanceEngine: ä¸è‡ªèº«Nicheçš„ç›¸å…³åº¦è¯„ä¼°
- OpportunityScorer: å‚ä¸æ—¶æœºè¯„åˆ† (æ—©æœŸ=é«˜åˆ†, è¡°é€€=ä½åˆ†)
- TrendAlert: è¶‹åŠ¿é¢„è­¦ + ä¼˜å…ˆçº§åˆ†çº§
- TrendHistory: å†å²è¶‹åŠ¿å½’æ¡£ + å¤ç°æ£€æµ‹
"""

import json
import logging
import math
import sqlite3
import threading
from collections import Counter, defaultdict, deque
from dataclasses import dataclass, field, asdict
from datetime import datetime, timezone, timedelta
from enum import Enum
from typing import Dict, List, Optional, Tuple, Set, Deque

logger = logging.getLogger(__name__)


# â”€â”€ æ•°æ®æ¨¡å‹ â”€â”€


class TrendPhase(Enum):
    """è¶‹åŠ¿é˜¶æ®µ"""
    EMERGING = "emerging"       # èŒèŠ½æœŸ (æœ€ä½³å‚ä¸æ—¶æœº)
    RISING = "rising"           # ä¸Šå‡æœŸ
    PEAKING = "peaking"         # é«˜å³°æœŸ
    DECLINING = "declining"     # è¡°é€€æœŸ
    DEAD = "dead"               # å·²è¿‡æ—¶


class TrendPriority(Enum):
    """è¶‹åŠ¿ä¼˜å…ˆçº§"""
    CRITICAL = "critical"       # å¿…é¡»ç«‹å³å‚ä¸
    HIGH = "high"               # å¼ºçƒˆå»ºè®®å‚ä¸
    MEDIUM = "medium"           # å¯ä»¥å‚ä¸
    LOW = "low"                 # å¯å¿½ç•¥
    IRRELEVANT = "irrelevant"   # ä¸ç›¸å…³


@dataclass
class TrendItem:
    """è¶‹åŠ¿æ¡ç›®"""
    keyword: str
    volume: int = 0                 # å½“å‰æåŠé‡
    volume_change: float = 0.0      # æåŠé‡å˜åŒ–ç‡
    phase: str = "emerging"
    priority: str = "medium"
    relevance_score: float = 0.0    # ä¸nicheç›¸å…³åº¦ (0-1)
    opportunity_score: float = 0.0  # å‚ä¸æ—¶æœºè¯„åˆ† (0-1)
    first_seen: str = ""
    last_seen: str = ""
    peak_volume: int = 0
    sample_tweets: List[str] = field(default_factory=list)
    related_hashtags: List[str] = field(default_factory=list)
    category: str = ""

    def to_dict(self) -> Dict:
        return asdict(self)

    @classmethod
    def from_dict(cls, d: Dict) -> "TrendItem":
        known = {f.name for f in cls.__dataclass_fields__.values()}
        filtered = {k: v for k, v in d.items() if k in known}
        return cls(**filtered)


@dataclass
class TrendAlert:
    """è¶‹åŠ¿é¢„è­¦"""
    trend: TrendItem
    alert_type: str             # "new_trend", "phase_change", "volume_spike", "opportunity"
    message: str = ""
    created_at: str = ""
    acknowledged: bool = False

    def to_dict(self) -> Dict:
        d = asdict(self)
        d["trend"] = self.trend.to_dict()
        return d


# â”€â”€ çˆ†å‘æ£€æµ‹å™¨ â”€â”€


class BurstDetector:
    """åŸºäºZ-scoreçš„çˆ†å‘æ£€æµ‹"""

    def __init__(self, window_size: int = 24, z_threshold: float = 2.0):
        self.window_size = window_size
        self.z_threshold = z_threshold
        self._history: Dict[str, Deque[int]] = defaultdict(lambda: deque(maxlen=window_size))

    def add_observation(self, keyword: str, count: int):
        """æ·»åŠ è§‚æµ‹å€¼"""
        self._history[keyword].append(count)

    def is_burst(self, keyword: str, current_count: int) -> Tuple[bool, float]:
        """æ£€æµ‹æ˜¯å¦çˆ†å‘"""
        history = self._history.get(keyword, deque())
        if len(history) < 3:
            # æ•°æ®ä¸è¶³ï¼Œç”¨ç®€å•é˜ˆå€¼
            return current_count > 50, 0.0

        values = list(history)
        mean = sum(values) / len(values)
        if mean == 0:
            return current_count > 10, 0.0

        variance = sum((v - mean) ** 2 for v in values) / len(values)
        std = math.sqrt(variance) if variance > 0 else 1.0

        z_score = (current_count - mean) / std
        return z_score > self.z_threshold, round(z_score, 2)

    def get_trend_phase(self, keyword: str) -> TrendPhase:
        """åˆ¤æ–­è¶‹åŠ¿é˜¶æ®µ"""
        history = list(self._history.get(keyword, deque()))
        if len(history) < 3:
            return TrendPhase.EMERGING

        recent = history[-3:]
        older = history[:-3] if len(history) > 3 else history[:1]

        recent_avg = sum(recent) / len(recent)
        older_avg = sum(older) / len(older) if older else 0

        if older_avg == 0:
            return TrendPhase.EMERGING if recent_avg < 100 else TrendPhase.RISING

        growth = (recent_avg - older_avg) / older_avg

        # æœ€è¿‘3ä¸ªå€¼çš„è¶‹åŠ¿
        if len(recent) >= 3:
            if recent[-1] < recent[-2] < recent[-3]:
                return TrendPhase.DECLINING
            if recent[-1] > recent[-2] and recent[-2] > recent[-3]:
                if growth > 0.5:
                    return TrendPhase.RISING
                return TrendPhase.PEAKING

        if growth > 1.0:
            return TrendPhase.EMERGING
        elif growth > 0.3:
            return TrendPhase.RISING
        elif growth > -0.1:
            return TrendPhase.PEAKING
        elif growth > -0.5:
            return TrendPhase.DECLINING
        return TrendPhase.DEAD

    def volume_change_rate(self, keyword: str) -> float:
        """è®¡ç®—é‡å˜åŒ–ç‡"""
        history = list(self._history.get(keyword, deque()))
        if len(history) < 2:
            return 0.0
        prev = history[-2] if history[-2] > 0 else 1
        return (history[-1] - prev) / prev


# â”€â”€ ç›¸å…³åº¦å¼•æ“ â”€â”€


class RelevanceEngine:
    """è¶‹åŠ¿ä¸Nicheçš„ç›¸å…³åº¦è¯„ä¼°"""

    def __init__(self, niche_keywords: List[str] = None, niche_hashtags: List[str] = None):
        self.niche_keywords = [k.lower() for k in (niche_keywords or [])]
        self.niche_hashtags = [h.lower().lstrip("#") for h in (niche_hashtags or [])]

    def score(self, trend: TrendItem) -> float:
        """è¯„ä¼°è¶‹åŠ¿ç›¸å…³åº¦"""
        if not self.niche_keywords and not self.niche_hashtags:
            return 0.5  # æ— nicheé…ç½®æ—¶ç»™ä¸­æ€§åˆ†

        score = 0.0
        keyword_lower = trend.keyword.lower()

        # ç›´æ¥åŒ¹é…
        if keyword_lower in self.niche_keywords:
            score += 0.5
        if keyword_lower.lstrip("#") in self.niche_hashtags:
            score += 0.5

        # éƒ¨åˆ†åŒ¹é…
        for nk in self.niche_keywords:
            if nk in keyword_lower or keyword_lower in nk:
                score += 0.2
                break

        # ç›¸å…³hashtagåŒ¹é…
        for rh in trend.related_hashtags:
            rh_lower = rh.lower().lstrip("#")
            if rh_lower in self.niche_hashtags or rh_lower in self.niche_keywords:
                score += 0.1

        # æ ·æœ¬æ¨æ–‡å†…å®¹åŒ¹é…
        if trend.sample_tweets:
            match_count = 0
            for tweet in trend.sample_tweets[:5]:
                tweet_lower = tweet.lower()
                if any(kw in tweet_lower for kw in self.niche_keywords):
                    match_count += 1
            if trend.sample_tweets:
                score += 0.2 * (match_count / min(5, len(trend.sample_tweets)))

        return min(1.0, score)


# â”€â”€ æ—¶æœºè¯„åˆ† â”€â”€


class OpportunityScorer:
    """å‚ä¸æ—¶æœºè¯„åˆ†å™¨"""

    PHASE_SCORES = {
        TrendPhase.EMERGING: 1.0,
        TrendPhase.RISING: 0.8,
        TrendPhase.PEAKING: 0.5,
        TrendPhase.DECLINING: 0.2,
        TrendPhase.DEAD: 0.0,
    }

    def score(self, trend: TrendItem, phase: TrendPhase, relevance: float) -> float:
        """ç»¼åˆæ—¶æœºè¯„åˆ†"""
        phase_score = self.PHASE_SCORES.get(phase, 0.5)

        # ç›¸å…³åº¦åŠ æƒ
        weighted = phase_score * 0.4 + relevance * 0.4

        # é‡çº§åŠ æˆ (å¤§è¶‹åŠ¿æ›´å€¼å¾—å‚ä¸)
        volume_bonus = min(0.2, math.log1p(trend.volume) / 20)
        weighted += volume_bonus

        return round(min(1.0, weighted), 3)

    def get_priority(self, opportunity_score: float, relevance: float) -> TrendPriority:
        """ç¡®å®šä¼˜å…ˆçº§"""
        combined = opportunity_score * 0.6 + relevance * 0.4

        if combined >= 0.8:
            return TrendPriority.CRITICAL
        elif combined >= 0.6:
            return TrendPriority.HIGH
        elif combined >= 0.4:
            return TrendPriority.MEDIUM
        elif combined >= 0.2:
            return TrendPriority.LOW
        return TrendPriority.IRRELEVANT


# â”€â”€ è¶‹åŠ¿å†å² â”€â”€


class TrendHistory:
    """è¶‹åŠ¿å†å²å½’æ¡£"""

    def __init__(self, db_path: str = "trend_history.db"):
        self.db_path = db_path
        self._local = threading.local()
        self._init_db()

    def _get_conn(self) -> sqlite3.Connection:
        if not hasattr(self._local, "conn") or self._local.conn is None:
            self._local.conn = sqlite3.connect(self.db_path)
            self._local.conn.row_factory = sqlite3.Row
            self._local.conn.execute("PRAGMA journal_mode=WAL")
        return self._local.conn

    def _init_db(self):
        conn = self._get_conn()
        conn.executescript("""
            CREATE TABLE IF NOT EXISTS trends (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                keyword TEXT NOT NULL,
                volume INTEGER DEFAULT 0,
                volume_change REAL DEFAULT 0,
                phase TEXT DEFAULT 'emerging',
                relevance_score REAL DEFAULT 0,
                opportunity_score REAL DEFAULT 0,
                priority TEXT DEFAULT 'medium',
                related_hashtags TEXT DEFAULT '[]',
                category TEXT DEFAULT '',
                recorded_at TEXT NOT NULL DEFAULT (datetime('now'))
            );
            CREATE TABLE IF NOT EXISTS trend_alerts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                keyword TEXT NOT NULL,
                alert_type TEXT NOT NULL,
                message TEXT DEFAULT '',
                acknowledged INTEGER DEFAULT 0,
                created_at TEXT NOT NULL DEFAULT (datetime('now'))
            );
            CREATE INDEX IF NOT EXISTS idx_trends_kw ON trends(keyword);
            CREATE INDEX IF NOT EXISTS idx_trends_time ON trends(recorded_at);
            CREATE INDEX IF NOT EXISTS idx_trends_phase ON trends(phase);
            CREATE INDEX IF NOT EXISTS idx_alerts_ack ON trend_alerts(acknowledged);
        """)
        conn.commit()

    def record(self, trend: TrendItem):
        """è®°å½•è¶‹åŠ¿å¿«ç…§"""
        conn = self._get_conn()
        conn.execute(
            """INSERT INTO trends(keyword, volume, volume_change, phase, relevance_score,
               opportunity_score, priority, related_hashtags, category)
               VALUES(?,?,?,?,?,?,?,?,?)""",
            (
                trend.keyword, trend.volume, trend.volume_change, trend.phase,
                trend.relevance_score, trend.opportunity_score, trend.priority,
                json.dumps(trend.related_hashtags), trend.category,
            ),
        )
        conn.commit()

    def add_alert(self, alert: TrendAlert) -> int:
        """æ·»åŠ é¢„è­¦"""
        conn = self._get_conn()
        cursor = conn.execute(
            "INSERT INTO trend_alerts(keyword, alert_type, message) VALUES(?,?,?)",
            (alert.trend.keyword, alert.alert_type, alert.message),
        )
        conn.commit()
        return cursor.lastrowid

    def acknowledge_alert(self, alert_id: int):
        """ç¡®è®¤é¢„è­¦"""
        conn = self._get_conn()
        conn.execute("UPDATE trend_alerts SET acknowledged=1 WHERE id=?", (alert_id,))
        conn.commit()

    def get_unacknowledged(self, limit: int = 20) -> List[Dict]:
        """è·å–æœªç¡®è®¤é¢„è­¦"""
        conn = self._get_conn()
        rows = conn.execute(
            "SELECT * FROM trend_alerts WHERE acknowledged=0 ORDER BY created_at DESC LIMIT ?",
            (limit,),
        ).fetchall()
        return [dict(r) for r in rows]

    def get_trend_history(self, keyword: str, days: int = 7) -> List[Dict]:
        """è·å–å…³é”®è¯å†å²"""
        conn = self._get_conn()
        cutoff = (datetime.now(timezone.utc) - timedelta(days=days)).isoformat()
        rows = conn.execute(
            "SELECT * FROM trends WHERE keyword=? AND recorded_at>=? ORDER BY recorded_at",
            (keyword, cutoff),
        ).fetchall()
        return [dict(r) for r in rows]

    def find_recurring(self, min_occurrences: int = 3, days: int = 90) -> List[Dict]:
        """å‘ç°å‘¨æœŸæ€§è¶‹åŠ¿"""
        conn = self._get_conn()
        cutoff = (datetime.now(timezone.utc) - timedelta(days=days)).isoformat()
        rows = conn.execute(
            """SELECT keyword, COUNT(DISTINCT date(recorded_at)) as occurrence_days,
                      MAX(volume) as max_volume, AVG(relevance_score) as avg_relevance
               FROM trends WHERE recorded_at>=?
               GROUP BY keyword HAVING occurrence_days>=?
               ORDER BY occurrence_days DESC""",
            (cutoff, min_occurrences),
        ).fetchall()
        return [dict(r) for r in rows]

    def hot_keywords(self, hours: int = 24, limit: int = 20) -> List[Dict]:
        """æœ€è¿‘çƒ­è¯"""
        conn = self._get_conn()
        cutoff = (datetime.now(timezone.utc) - timedelta(hours=hours)).isoformat()
        rows = conn.execute(
            """SELECT keyword, MAX(volume) as max_volume, MAX(opportunity_score) as best_opportunity,
                      COUNT(*) as snapshots
               FROM trends WHERE recorded_at>=?
               GROUP BY keyword ORDER BY max_volume DESC LIMIT ?""",
            (cutoff, limit),
        ).fetchall()
        return [dict(r) for r in rows]

    def stats(self) -> Dict:
        """ç»Ÿè®¡"""
        conn = self._get_conn()
        total_trends = conn.execute("SELECT COUNT(DISTINCT keyword) FROM trends").fetchone()[0]
        total_snapshots = conn.execute("SELECT COUNT(*) FROM trends").fetchone()[0]
        total_alerts = conn.execute("SELECT COUNT(*) FROM trend_alerts").fetchone()[0]
        unack_alerts = conn.execute("SELECT COUNT(*) FROM trend_alerts WHERE acknowledged=0").fetchone()[0]
        return {
            "unique_trends": total_trends,
            "total_snapshots": total_snapshots,
            "total_alerts": total_alerts,
            "unacknowledged_alerts": unack_alerts,
        }


# â”€â”€ ç»„åˆæ¥å£ â”€â”€


class TrendTracker:
    """è¶‹åŠ¿è¿½è¸ªå¼•æ“ â€” ç»Ÿä¸€å…¥å£"""

    def __init__(
        self,
        niche_keywords: List[str] = None,
        niche_hashtags: List[str] = None,
        db_dir: str = ".",
        burst_window: int = 24,
        burst_threshold: float = 2.0,
    ):
        self.burst_detector = BurstDetector(burst_window, burst_threshold)
        self.relevance = RelevanceEngine(niche_keywords, niche_hashtags)
        self.opportunity = OpportunityScorer()
        self.history = TrendHistory(f"{db_dir}/trend_history.db")
        self._active_trends: Dict[str, TrendItem] = {}

    def process_trending(self, trending_data: List[Dict]) -> List[TrendItem]:
        """å¤„ç†è¶‹åŠ¿æ•°æ®"""
        results = []
        now = datetime.now(timezone.utc).isoformat()

        for data in trending_data:
            keyword = data.get("keyword", data.get("name", ""))
            volume = data.get("volume", data.get("tweet_volume", 0)) or 0

            if not keyword:
                continue

            # çˆ†å‘æ£€æµ‹
            self.burst_detector.add_observation(keyword, volume)
            is_burst, z_score = self.burst_detector.is_burst(keyword, volume)

            # é˜¶æ®µåˆ¤æ–­
            phase = self.burst_detector.get_trend_phase(keyword)

            # æ„å»ºè¶‹åŠ¿æ¡ç›®
            trend = TrendItem(
                keyword=keyword,
                volume=volume,
                volume_change=self.burst_detector.volume_change_rate(keyword),
                phase=phase.value,
                first_seen=self._active_trends.get(keyword, TrendItem(keyword=keyword)).first_seen or now,
                last_seen=now,
                peak_volume=max(volume, self._active_trends.get(keyword, TrendItem(keyword=keyword)).peak_volume),
                sample_tweets=data.get("sample_tweets", [])[:5],
                related_hashtags=data.get("related_hashtags", data.get("related", []))[:10],
                category=data.get("category", ""),
            )

            # ç›¸å…³åº¦è¯„åˆ†
            trend.relevance_score = self.relevance.score(trend)

            # æ—¶æœºè¯„åˆ†
            trend.opportunity_score = self.opportunity.score(trend, phase, trend.relevance_score)

            # ä¼˜å…ˆçº§
            priority = self.opportunity.get_priority(trend.opportunity_score, trend.relevance_score)
            trend.priority = priority.value

            # ç”Ÿæˆé¢„è­¦
            old_trend = self._active_trends.get(keyword)
            alerts = self._check_alerts(trend, old_trend, is_burst, z_score)
            for alert in alerts:
                self.history.add_alert(alert)

            # è®°å½•å†å²
            self.history.record(trend)

            # æ›´æ–°æ´»è·ƒè¶‹åŠ¿
            self._active_trends[keyword] = trend
            results.append(trend)

        # æŒ‰æ—¶æœºè¯„åˆ†æ’åº
        results.sort(key=lambda t: t.opportunity_score, reverse=True)
        return results

    def _check_alerts(
        self, trend: TrendItem, old: Optional[TrendItem], is_burst: bool, z_score: float
    ) -> List[TrendAlert]:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦é¢„è­¦"""
        alerts = []
        now = datetime.now(timezone.utc).isoformat()

        # æ–°è¶‹åŠ¿
        if old is None and trend.relevance_score > 0.3:
            alerts.append(TrendAlert(
                trend=trend,
                alert_type="new_trend",
                message=f"ğŸ†• New relevant trend: {trend.keyword} (relevance: {trend.relevance_score:.2f})",
                created_at=now,
            ))

        # çˆ†å‘
        if is_burst and z_score > 3.0:
            alerts.append(TrendAlert(
                trend=trend,
                alert_type="volume_spike",
                message=f"ğŸ“ˆ Volume spike: {trend.keyword} z-score={z_score} volume={trend.volume}",
                created_at=now,
            ))

        # é˜¶æ®µå˜åŒ–
        if old and old.phase != trend.phase:
            alerts.append(TrendAlert(
                trend=trend,
                alert_type="phase_change",
                message=f"ğŸ”„ Phase change: {trend.keyword} {old.phase} â†’ {trend.phase}",
                created_at=now,
            ))

        # é«˜æ—¶æœºçª—å£
        if trend.opportunity_score > 0.8 and trend.relevance_score > 0.5:
            alerts.append(TrendAlert(
                trend=trend,
                alert_type="opportunity",
                message=f"ğŸ¯ High opportunity: {trend.keyword} score={trend.opportunity_score:.2f}",
                created_at=now,
            ))

        return alerts

    def get_active_trends(
        self,
        min_relevance: float = 0.0,
        phase: Optional[str] = None,
        priority: Optional[str] = None,
    ) -> List[TrendItem]:
        """è·å–æ´»è·ƒè¶‹åŠ¿"""
        trends = list(self._active_trends.values())

        if min_relevance > 0:
            trends = [t for t in trends if t.relevance_score >= min_relevance]
        if phase:
            trends = [t for t in trends if t.phase == phase]
        if priority:
            trends = [t for t in trends if t.priority == priority]

        trends.sort(key=lambda t: t.opportunity_score, reverse=True)
        return trends

    def get_actionable(self, top_n: int = 5) -> List[TrendItem]:
        """è·å–æœ€å€¼å¾—å‚ä¸çš„è¶‹åŠ¿"""
        trends = self.get_active_trends(min_relevance=0.3)
        # è¿‡æ»¤æ‰å·²è¿‡æ—¶çš„
        actionable = [t for t in trends if t.phase not in ("dead", "declining")]
        return actionable[:top_n]

    def suggest_content(self, trend: TrendItem) -> List[Dict]:
        """å»ºè®®å‚ä¸å†…å®¹"""
        suggestions = []

        # åŸºäºè¶‹åŠ¿é˜¶æ®µ
        if trend.phase == "emerging":
            suggestions.append({
                "type": "first_mover",
                "prompt": f"Write a thought-leadership tweet about {trend.keyword} before it goes mainstream",
                "timing": "post immediately",
            })
        elif trend.phase == "rising":
            suggestions.append({
                "type": "hot_take",
                "prompt": f"Share your unique perspective on the trending topic: {trend.keyword}",
                "timing": "post within 2 hours",
            })
        elif trend.phase == "peaking":
            suggestions.append({
                "type": "thread",
                "prompt": f"Write a comprehensive thread about {trend.keyword} with data and insights",
                "timing": "post within 4 hours",
            })

        # é€šç”¨å»ºè®®
        if trend.related_hashtags:
            top_tags = trend.related_hashtags[:5]
            suggestions.append({
                "type": "hashtag_ride",
                "prompt": f"Create content using these trending hashtags: {', '.join(top_tags)}",
                "timing": "flexible",
            })

        return suggestions

    def export_report(self, trends: List[TrendItem], format: str = "text") -> str:
        """å¯¼å‡ºè¶‹åŠ¿æŠ¥å‘Š"""
        if format == "json":
            return json.dumps([t.to_dict() for t in trends], indent=2, default=str)
        elif format == "text":
            lines = [f"ğŸ“Š Trend Report ({len(trends)} trends)", "=" * 50]
            for i, t in enumerate(trends, 1):
                phase_emoji = {
                    "emerging": "ğŸŒ±", "rising": "ğŸ“ˆ", "peaking": "ğŸ”¥",
                    "declining": "ğŸ“‰", "dead": "ğŸ’€",
                }.get(t.phase, "â“")
                prio_emoji = {
                    "critical": "ğŸš¨", "high": "ğŸ”´", "medium": "ğŸŸ¡",
                    "low": "ğŸŸ¢", "irrelevant": "âšª",
                }.get(t.priority, "âšª")
                lines.append(
                    f"#{i} {phase_emoji} {prio_emoji} {t.keyword} "
                    f"[Vol:{t.volume} Î”{t.volume_change:+.1%}]"
                )
                lines.append(
                    f"   Relevance:{t.relevance_score:.2f} "
                    f"Opportunity:{t.opportunity_score:.2f} "
                    f"Phase:{t.phase}"
                )
                if t.related_hashtags:
                    lines.append(f"   Tags: {', '.join(t.related_hashtags[:5])}")
                lines.append("")
            return "\n".join(lines)
        return ""

    def stats(self) -> Dict:
        """ç»Ÿè®¡"""
        active = len(self._active_trends)
        by_phase = defaultdict(int)
        by_priority = defaultdict(int)
        for t in self._active_trends.values():
            by_phase[t.phase] += 1
            by_priority[t.priority] += 1

        db_stats = self.history.stats()
        return {
            "active_trends": active,
            "by_phase": dict(by_phase),
            "by_priority": dict(by_priority),
            **db_stats,
        }
